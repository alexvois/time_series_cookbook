# -*- coding: utf-8 -*-
"""time-series-pytorch1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1myI1xgwXV4dL6qAgzP98_TtqrwBGU68S
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import yfinance as yf

# Load Stock Market Data from Yahoo Finance
def load_stock_data():
    return yf.download('AAPL', start='2010-01-01', end='2022-02-26')

def get_raw_series():
    df = load_stock_data()
    time_series = df['Close'].values  # Use Close price as time series
    return time_series

# Function to create sequences
def create_sequences(data, seq_length):
    xs = []
    ys = []
    for i in range(len(data)-seq_length):
        x = data[i:(i+seq_length)]
        y = data[i+seq_length]
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

seq_length = 50
# Load raw series
data_raw = get_raw_series()

# Set train length (in number of sequences)
train_length = 2800

# Fit scaler on TRAIN RANGE ONLY (include initial seq_length context used in training windows)
values_for_scaler = data_raw[:train_length + seq_length]
_scaler = MinMaxScaler().fit(values_for_scaler.reshape(-1, 1))
# Scale entire series using train-fitted scaler
data = _scaler.transform(data_raw.reshape(-1, 1)).flatten()

# Create sequences on scaled data
X, y = create_sequences(data, seq_length)
print()
print(f"created {len(X)} samples")

# Convert data to PyTorch tensors
X_train, y_train = torch.tensor(X[:train_length, :, None], dtype=torch.float32), torch.tensor(y[:train_length, None], dtype=torch.float32)
X_test, y_test = torch.tensor(X[train_length:, :, None], dtype=torch.float32), torch.tensor(y[train_length:, None], dtype=torch.float32)
print(f"train samples: {len(X_train)}")
print(f"test samples: {len(X_test)}")

class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):
        super(LSTMModel, self).__init__()
        self.hidden_dim = hidden_dim
        self.layer_dim = layer_dim
        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x, h0=None, c0=None):
        # If hidden and cell states are not provided, initialize them as zeros
        if h0 is None or c0 is None:
            h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)
            c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)

        # Forward pass through LSTM
        out, (hn, cn) = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])  # Selecting the last output
        return out, hn, cn

# Initialize model, loss, and optimizer
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)
model = LSTMModel(input_dim=1, hidden_dim=50, layer_dim=1, output_dim=1).to(device)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Set random seed for reproducibility
np.random.seed(0)
torch.manual_seed(0)

# Training loop
num_epochs = 40
h0, c0 = None, None  # Initialize hidden and cell states

X_train = X_train.to(device)
y_train = y_train.to(device)
X_test = X_test.to(device)
y_test = y_test.to(device)

for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()

    # Forward pass
    outputs, h0, c0 = model(X_train, h0, c0)

    # Compute loss
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()

    # Detach hidden and cell states to prevent backpropagation through the entire sequence
    h0 = h0.detach()
    c0 = c0.detach()

    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# Predicted outputs
print("Making predictions (sliding window, 1-step ahead)...")
model.eval()

h0 = torch.zeros(model.layer_dim, X_train.size(0), model.hidden_dim).to(device)
c0 = torch.zeros(model.layer_dim, X_train.size(0), model.hidden_dim).to(device)
predicted_train, _, _ = model(X_train, h0, c0)
original_train = data[seq_length:train_length+seq_length]  # Original data from the end of the first sequence
time_steps_train = np.arange(seq_length, train_length + seq_length)  # Corresponding time steps

h0 = torch.zeros(model.layer_dim, X_test.size(0), model.hidden_dim).to(device)
c0 = torch.zeros(model.layer_dim, X_test.size(0), model.hidden_dim).to(device)
predicted_test, _, _ = model(X_test, h0, c0)
original_test = data[train_length+seq_length:train_length+seq_length+len(X_test)]
time_steps_test = np.arange(train_length + seq_length, train_length + seq_length + len(X_test))  # Corresponding time steps

# Metrics (on scaled values)
y_train_np = y_train.detach().cpu().numpy().flatten()
y_test_np = y_test.detach().cpu().numpy().flatten()
predicted_train_np = predicted_train.detach().cpu().numpy().flatten()
predicted_test_np = predicted_test.detach().cpu().numpy().flatten()

train_mse = mean_squared_error(y_train_np, predicted_train_np)
train_mae = mean_absolute_error(y_train_np, predicted_train_np)
test_mse = mean_squared_error(y_test_np, predicted_test_np)
test_mae = mean_absolute_error(y_test_np, predicted_test_np)

print(f"Train MSE: {train_mse:.6f}")
print(f"Train MAE: {train_mae:.6f}")
print(f"Test MSE: {test_mse:.6f}")
print(f"Test MAE: {test_mae:.6f}")

# Plotting
plt.figure(figsize=(12, 6))
plt.plot(time_steps_train, original_train, label='Original Train Data')
plt.plot(time_steps_train, predicted_train.cpu().detach().numpy(), label='Predicted Train Data', linestyle='--')
plt.plot(time_steps_test, original_test, label='Original Test Data')
plt.plot(time_steps_test, predicted_test.cpu().detach().numpy(), label='Predicted Test Data', linestyle='--')
plt.title('LSTM Model Predictions vs. Original Data')
plt.xlabel('Time Step')
plt.ylabel('Value')
plt.legend()
plt.show()
